# 28. Reinforcement Learning Trading Agent (RL-Layer)

## üéØ –¶–µ–ª—å —Ä–∞–∑–¥–µ–ª–∞

–î–æ–±–∞–≤–∏—Ç—å –∫ —Å–∏—Å—Ç–µ–º–µ MarketFlow —Å–ª–æ–π –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º (Reinforcement Learning, RL),  
—á—Ç–æ–±—ã —Ç–æ—Ä–≥–æ–≤—ã–µ –∞–≥–µ–Ω—Ç—ã –º–æ–≥–ª–∏ **—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é**  
–Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø—ã—Ç–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å —Ä—ã–Ω–∫–æ–º –∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö —Å–¥–µ–ª–æ–∫.

---

## üß≠ 28.1. –ö–æ–Ω—Ü–µ–ø—Ü–∏—è RL –≤ —Ç—Ä–µ–π–¥–∏–Ω–≥–µ

| –≠–ª–µ–º–µ–Ω—Ç | –ó–Ω–∞—á–µ–Ω–∏–µ |
|----------|-----------|
| **Environment (—Å—Ä–µ–¥–∞)** | –†—ã–Ω–æ–∫: OHLCV, sentiment, —Å–∏–≥–Ω–∞–ª—ã, —Ä–∏—Å–∫ |
| **Agent (–∞–≥–µ–Ω—Ç)** | –¢–æ—Ä–≥–æ–≤—ã–π –±–æ—Ç, –ø—Ä–∏–Ω–∏–º–∞—é—â–∏–π —Ä–µ—à–µ–Ω–∏–µ BUY/SELL/HOLD |
| **State (—Å–æ—Å—Ç–æ—è–Ω–∏–µ)** | –ù–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏ |
| **Action (–¥–µ–π—Å—Ç–≤–∏–µ)** | BUY / SELL / HOLD |
| **Reward (–Ω–∞–≥—Ä–∞–¥–∞)** | –ü—Ä–∏–±—ã–ª—å ‚Äì —à—Ç—Ä–∞—Ñ –∑–∞ —Ä–∏—Å–∫, latency, drawdown |
| **Policy (œÄ)** | –ü–æ–≤–µ–¥–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–æ–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ |

---

## ‚öôÔ∏è 28.2. –û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RL-—É—Ä–æ–≤–Ω—è

```

[Market Data + Features]
‚Üì
[RL Agent ‚Üí Decision ‚Üí Market Reaction]
‚Üì
[Reward Calculation ‚Üí Model Update]
‚Üì
[Checkpoint / Retrain / Deploy]

````

---

## üß† 28.3. –ü—Ä–∏–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è (state vector)

–°–æ—Å—Ç–æ—è–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:

```text
[ close, volume, volatility, momentum, sentiment, drawdown, position_flag ]
````

–ø—Ä–∏–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è:

```
s_t = [0.486, 1.9M, 0.0072, +0.35, +0.22, 0.04, 1]
```

---

## üß© 28.4. –ü—Ä–∏–º–µ—Ä —Å—Ä–µ–¥—ã –¥–ª—è RL (Gym Environment)

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class MarketEnv(gym.Env):
    def __init__(self, df):
        super().__init__()
        self.df = df
        self.i = 0
        self.balance = 1000
        self.position = 0
        self.action_space = spaces.Discrete(3)  # BUY=0, SELL=1, HOLD=2
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(7,))
    
    def reset(self, seed=None):
        self.i, self.balance, self.position = 0, 1000, 0
        return self._get_state(), {}
    
    def _get_state(self):
        r = self.df.iloc[self.i]
        return np.array([r.close, r.volume, r.volatility, r.momentum,
                         r.sentiment, r.drawdown, self.position])
    
    def step(self, action):
        r = self.df.iloc[self.i]
        reward = 0
        if action == 0 and self.position == 0:  # BUY
            self.position = 1
            self.entry = r.close
        elif action == 1 and self.position == 1:  # SELL
            reward = (r.close - self.entry) / self.entry * 100
            self.balance *= (1 + reward / 100)
            self.position = 0
        self.i += 1
        done = self.i >= len(self.df) - 1
        return self._get_state(), reward, done, False, {}
```

---

## üßÆ 28.5. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ (DQN / PPO –ø—Ä–∏–º–µ—Ä)

```python
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from market_env import MarketEnv
import pandas as pd

df = pd.read_csv("training_data.csv")
env = DummyVecEnv([lambda: MarketEnv(df)])
model = PPO("MlpPolicy", env, verbose=1, tensorboard_log="./rl_logs/")

model.learn(total_timesteps=200_000)
model.save("rl_agent_marketflow")
```

üìò –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –æ–±—É—á–∞—Ç—å—Å—è –∫–∞–∫ –ª–æ–∫–∞–ª—å–Ω–æ, —Ç–∞–∫ –∏ –≤ AKS –ø—Ä–∏ –ø–æ–º–æ—â–∏ GPU-–Ω–æ–¥.

---

## üìä 28.6. Reward-—Ñ—É–Ω–∫—Ü–∏—è (–±–∞–ª–∞–Ω—Å –ø—Ä–∏–±—ã–ª—å/—Ä–∏—Å–∫)

–ü—Ä–∏–º–µ—Ä –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–≥—Ä–∞–¥—ã:

```
reward = profit_pct - 0.5 * abs(drawdown) - 0.2 * transaction_cost
```

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç          | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ                                    |
| ------------------ | --------------------------------------------- |
| `profit_pct`       | –ø—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫ —Å–¥–µ–ª–∫–∏                         |
| `drawdown`         | —à—Ç—Ä–∞—Ñ –∑–∞ —Ä–∏—Å–∫                                 |
| `transaction_cost` | –∫–æ–º–∏—Å—Å–∏—è –∏ –ø—Ä–æ—Å–∫–∞–ª—å–∑—ã–≤–∞–Ω–∏–µ                    |
| –≤–µ—Å `Œ±`            | –±–∞–ª–∞–Ω—Å–∏—Ä—É–µ—Ç –º–µ–∂–¥—É –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å—é –∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å—é |

---

## üß† 28.7. Online Learning –∏ Retraining

MarketFlow RL Agent –º–æ–∂–µ—Ç:

* –¥–æ–æ–±—É—á–∞—Ç—å—Å—è –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (`partial_fit`);
* –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∫–∞–∂–¥—ã–µ N —á–∞—Å–æ–≤;
* —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π (`rl_agent_v1`, `rl_agent_v2`, ‚Ä¶).

**Workflow (n8n / Airflow):**

```
üïí Cron (–µ–∂–µ–¥–Ω–µ–≤–Ω–æ)
  ‚Üí üü® Load new trades (Snowflake)
  ‚Üí üü¶ Retrain RL Agent
  ‚Üí üü• Push updated model to AKS
  ‚Üí üüß Notify via Telegram
```

---

## ü§ñ 28.8. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è RL-Agent —Å Swarm Layer

–ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è RL Agent —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –µ—â—ë –æ–¥–Ω–∏–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–º —Ä–æ—è:

| –ê–≥–µ–Ω—Ç        | –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è               | –í–µ—Å     |
| ------------ | --------------------------- | ------- |
| SmartMoney   | —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–∏–≥–Ω–∞–ª—ã         | 0.3     |
| Sentiment    | NLP/–Ω–æ–≤–æ—Å—Ç–∏                 | 0.2     |
| Risk         | –∫–æ–Ω—Ç—Ä–æ–ª—å –ª–∏–º–∏—Ç–æ–≤            | 0.2     |
| Tuner        | –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤      | 0.1     |
| **RL Agent** | –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π | **0.2** |

–†–µ–∑—É–ª—å—Ç–∞—Ç: —Ä–µ—à–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É—é—Ç—Å—è –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ.

---

## üß© 28.9. –ú–µ—Ç—Ä–∏–∫–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ RL

| –ú–µ—Ç—Ä–∏–∫–∞                  | –û–ø–∏—Å–∞–Ω–∏–µ                       |
| ------------------------ | ------------------------------ |
| **Average Reward**       | —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ —ç–ø–∏–∑–æ–¥      |
| **Sharpe Ratio (RL)**    | —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏        |
| **WinRate (RL)**         | –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö —ç–ø–∏–∑–æ–¥–æ–≤    |
| **Learning Progress**    | —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ–ª–∏—Ç–∏–∫–∏ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º |
| **Exploration Rate (Œµ)** | –¥–æ–ª—è —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π        |

---

## üìà 28.10. –ü—Ä–∏–º–µ—Ä –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è

```python
import matplotlib.pyplot as plt
import pandas as pd

log = pd.read_csv("rl_logs/progress.csv")
plt.plot(log["timesteps"], log["episode_reward_mean"])
plt.title("Reinforcement Learning ‚Äî Reward Dynamics")
plt.xlabel("Steps")
plt.ylabel("Mean Reward")
plt.grid()
plt.show()
```

---

## ‚öôÔ∏è 28.11. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è RL-Agent –≤ Decision Hub

RL-–º–æ–¥–µ–ª—å –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫–∞–∫ –≤–Ω–µ—à–Ω–∏–π –º–æ–¥—É–ª—å:

```python
from stable_baselines3 import PPO
model = PPO.load("rl_agent_marketflow")

obs = env.reset()
action, _ = model.predict(obs)
decision = ["BUY", "SELL", "HOLD"][int(action)]
```

–≠—Ç–∏ —Ä–µ—à–µ–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –≤ Kafka-—Ç–æ–ø–∏–∫ `rl_decisions`,
–≥–¥–µ Decision Hub –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö —Å –¥—Ä—É–≥–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏.

---

## üì¶ 28.12. –†–∞–∑–≤—ë—Ä—Ç—ã–≤–∞–Ω–∏–µ –≤ AKS

* –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä `marketflow-rl-agent`
  (—Å Python + Gym + Stable-Baselines3 + Snowflake Connector)
* Helm values:

```yaml
image:
  repository: marketflowregistry.azurecr.io/rl-agent
  tag: v1.0
resources:
  limits:
    cpu: "1"
    memory: "2Gi"
```

---

## üßÆ 28.13. –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è

| –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ                  | –û–ø–∏—Å–∞–Ω–∏–µ                                                 |
| --------------------------- | -------------------------------------------------------- |
| **Multi-Asset RL**          | –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ö (BTC, ETH, SOL, WIF) |
| **Hierarchical RL**         | –∞–≥–µ–Ω—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä, —É–ø—Ä–∞–≤–ª—è—é—â–∏–π –ø–æ–¥–∞–≥–µ–Ω—Ç–∞–º–∏                  |
| **Meta-RL**                 | –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –ø–µ—Ä–µ–Ω–æ—Å–∏—Ç—å –æ–ø—ã—Ç –º–µ–∂–¥—É —Ä–∞–∑–Ω—ã–º–∏ —Ä—ã–Ω–∫–∞–º–∏    |
| **Continuous Action Space** | —á–∞—Å—Ç–∏—á–Ω–∞—è –ø—Ä–æ–¥–∞–∂–∞ / –¥–æ–ª–∏–≤–∫–∞ –ø–æ–∑–∏—Ü–∏–∏                      |
| **Self-Play Simulation**    | RL –ø—Ä–æ—Ç–∏–≤ —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ ‚Äú—Ä—ã–Ω–∫–∞‚Äù (–≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞)   |

---

## üöÄ 28.14. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–Ω–µ–¥—Ä–µ–Ω–∏—è RL-Layer

‚úÖ –ê–≥–µ–Ω—Ç —É—á–∏—Ç—Å—è –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º —Ä—ã–Ω–∫–∞
‚úÖ –°–∏—Å—Ç–µ–º–∞ –ø–æ–ª—É—á–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ —Ä–µ–∞–∫—Ü–∏—é
‚úÖ MarketFlow —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–∞–º–æ–æ–±—É—á–∞—é—â–µ–π—Å—è
‚úÖ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ re-train –∏ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π

üìä **–ö–ª—é—á–µ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å:** ‚ÄúAI Decision Accuracy‚Äù —Ä–∞—Å—Ç—ë—Ç —Å 74% ‚Üí 85%
–ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ RL-–∞–≥–µ–Ω—Ç–∞ –≤ —Å–≤—è–∑–∫–µ —Å–æ Swarm Layer.
